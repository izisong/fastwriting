#encoding=utf-8
from django.http import Http404, HttpResponse
from django.shortcuts import render_to_response
import redis
import jieba
import os, fileinput, glob
from fastwriting import settings

redis_client = redis.StrictRedis(host='localhost', port=6379, db=0)

def index(request):
    return render_to_response('index.html', locals())

def import_blog(request):
    if 'blog_site' in request.GET:
		blog_site = request.GET['blog_site']
		import_blog(blog_site)
		message = 'hello'
    else:
        message = 'You submitted an empty form.'
    return HttpResponse(message)

def deal_blog(request):
    deal_blog()
    return HttpResponse('success')

def divide_articles(request):
	blog_path = settings.MEDIA_ROOT+'blog'
	bloger_list = os.listdir(blog_path)
	style_path = settings.MEDIA_ROOT+'style'
	style_list = os.listdir(style_path)
	for bloger in bloger_list:
		if (bloger+'_style.txt' in style_list) == False:
			bloger_path = os.path.join(blog_path, bloger)
			os.system('nohup '+settings.SHELL_ROOT+'DivideArticles.sh '+bloger_path+' '+style_path+' &')
	return HttpResponse()
	
def sentences_to_redis(request):
	pipe = redis_client.pipeline()
	count = 0
	for line in fileinput.input(glob.glob("data/chunyinStyle.txt")):
		if fileinput.isfirstline(): # first in a file?
			sys.stderredis_client.write("-- reading %s --\n" % fileinput.filename())
		#sys.stdout.write(str(fileinput.lineno()) + " " + line)
		count += 1
		pipe.hset('chunyinHash', count, line)
	pipe.execute()
	print 'sentences To Redis end'
	return HttpResponse()

def words_to_redis(request):
	pipe = redis_client.pipeline()
	count = 0
	wordsNum = 0
	ik = IKSegmenter(StringReader(""), 1);
	for line in fileinput.input(glob.glob("data/chunyinStyle.txt")):
		if fileinput.isfirstline(): # first in a file?
			sys.stderredis_client.write("-- reading %s --\n" % fileinput.filename())
		count += 1
		ik.reset(StringReader(unicode(line, 'utf-8')));
		lexeme = ik.next()
		while lexeme :
			word = lexeme.getLexemeText()
			if len(word)>1 :
				pipe.sadd(word, count)
				wordsNum += 1
			lexeme = ik.next()
	pipe.execute()
	print 'words To Redis end. the num of words:'+str(wordsNum)


def divide(request, sentence):
	word_list = jieba.cut(sentence, cut_all=True)
	valid_words = []
	for word in word_list:
		if len(word)>1:
			valid_words.append(word)
	if len(word_list)==0:
		return HttpResponse('')
	matched_sentence_ids = redis_client.sinter(valid_words)
	matched_sentences = []
	for sentence_id in matched_sentence_ids:
		matched_sentences.append(redis_client.hget('chunyinHash', sentence_id))
	return render_to_response('divide.html', locals())
